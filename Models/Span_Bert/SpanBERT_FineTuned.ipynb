{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpanBERT FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.16.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"mrm8488/spanbert-large-finetuned-squadv1\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df.iloc[:177542]\n",
    "# val = df.iloc[177542:202906]\n",
    "# test = df.iloc[202906:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(\"train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val.to_csv(\"val.csv\", index=False)\n",
    "# test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4f8eb56b2794af64\n",
      "Reusing dataset csv (/home/maha.agro/.cache/huggingface/datasets/csv/default-4f8eb56b2794af64/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4c0340bc4f437e946e42c67e7ccb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"val\": \"val.csv\", \"test\": \"test.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'answer', 'context', 'question', 'email_addresses', 'subject', 'body'],\n",
       "        num_rows: 177542\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['Unnamed: 0', 'answer', 'context', 'question', 'email_addresses', 'subject', 'body'],\n",
       "        num_rows: 25364\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'answer', 'context', 'question', 'email_addresses', 'subject', 'body'],\n",
       "        num_rows: 50726\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'answer': \"{'answer_start': [75], 'text': ['catherine34@gmail.com']}\",\n",
       " 'context': 'Alaska create an email with subject high speed internet access and send to catherine34@gmail.com and in the content please say 1 login pallen pw ke9davis i don t think these are required by the isp 2 static ip address ip 64 216 90 105 sub 255 255 255 248 gate 64 216 90 110 dns 151 164 1 8 3 company 0413 rc 105891',\n",
       " 'question': 'What are the email addresses?',\n",
       " 'email_addresses': 'catherine34@gmail.com',\n",
       " 'subject': 'high speed internet access',\n",
       " 'body': '1 login pallen pw ke9davis i don t think these are required by the isp 2 static ip address ip 64 216 90 105 sub 255 255 255 248 gate 64 216 90 110 dns 151 164 1 8 3 company 0413 rc 105891'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SpanBERT/spanbert-large-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1184, 1110, 1240, 1271, 136, 102, 1139, 1271, 1110, 188, 7777, 2497, 1394, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(dataset[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > max_length:\n",
    "        break\n",
    "example = dataset[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1810"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 384, 384, 384, 384, 384, 328]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] what are the email addresses? [SEP] alaska please write an email with topic texas puts reliability rules through paces and send it to amymiller @ hotmail. com with the body of the email dear energy insight subscribers if you cannot read this version of the 20 energy insight daily e mail please click on this link 20 change your 20 user preferences to reflect plain text e mail rather than html e mail tha nk 20 you for your patience if you have any questions feel free to contact us 20 at 1 800 424 2908 1 720 548 5700 if your are outside the u s or e mail us 20 at custserv ftenergy com image image image image image image image image image image image image image image 09 09 09 image 09 image 09 image 09 image 09 image 09 image 09 image 09 image 09 image 09 image 09 image 09 image 09 image 09 09 09 09 09 09 09 09 09 09updated may 15 2001 09 09 09 09 09 image 09 09 09 09 09 09 image 09 09 09 image 09 09 09 09 image 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09texas puts reliability rules through paces 09the texas public utility commission puc recently approved new reliabi lity 20 rules for the state s main power grid the goal was to scrutinize rules 20 governing other deregulated markets to see how well they have worked and 20 then find the best route for the electric reliability council of texas 20 ercot 09 09 09 image 09 image 09 09 09 09 09 09 09 09 09 image 09 09 09 09 image 09 image 09 09 09 09 09 09 09 image 09 09 09 09 09 09 image 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 [SEP]\n",
      "[CLS] what are the email addresses? [SEP] reliabi lity 20 rules for the state s main power grid the goal was to scrutinize rules 20 governing other deregulated markets to see how well they have worked and 20 then find the best route for the electric reliability council of texas 20 ercot 09 09 09 image 09 image 09 09 09 09 09 09 09 09 09 image 09 09 09 09 image 09 image 09 09 09 09 09 09 09 image 09 09 09 09 09 09 image 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 image 09 09 09 09 09 09 09 09capital markets love energy related firms 09energy companies dominate stock offerings ipos 09 09generators garner the most attention money 09 09good times bound to end 09 09 image 09 image 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 [SEP]\n",
      "[CLS] what are the email addresses? [SEP] 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 image 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 image 09 09 09 09 09 09 09 09capital markets love energy related firms 09energy companies dominate stock offerings ipos 09 09generators garner the most attention money 09 09good times bound to end 09 09 image 09 image 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 image 09 09 09 09 09 09 09mollusks create mayhem for power plants 09costs high to fight zebra mussels 09 09authorities warn of other damaging invaders 09 09 09 image 09 image 09 09 09 09 09 09 09 09 image 09 09 image 09 09 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 image 09 09 09 09 09 09 09gas use for power generation leveled out in 2000 09coal still fuel of choice 09 09value to balanced fuel portfolio 09 09 09 image 09 image 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09florida power outlines benefits of lat year 01 s merger 09go to [SEP]\n",
      "[CLS] what are the email addresses? [SEP] 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 image 09 09 09 09 09 09 09gas use for power generation leveled out in 2000 09coal still fuel of choice 09 09value to balanced fuel portfolio 09 09 09 image 09 image 09 09 09 09 09 09 09 09 image 09 09 09 image 09 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 09 09florida power outlines benefits of lat year 01 s merger 09go to full story 09 09nstar files with ferc for consumer protection order 09go to full story 09 09cpuc set to approve plan to repay state for power buys 09go to full story 09 09london electricity 01 s bid for seeboard rejected reports say 09go to full story 09 09tennessee gas announces open season for connexion project 09go to full story 09 09avista names ceo ely as chairman 09go to full story 20 09 09kerr mcgee announces 1 25b deal 09go to full story 09 09utility com to refund 70 000 to pa customers 09go to full story 09 09conoco to build 75m gas to liquids demonstration plant 09go to full story 09 09dpl to add 160 mw in ohio by 2002 09go to full story 09 09 09to view all of today s executive news headlines 20 09click here 20 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 image 09 image 09 image 09 09 09 09 09 09 09 09 09 image 09 image 09 09 copyright 2001 ft energy all rights reserved ft and financia l 20 times are trademarks of the financial [SEP]\n",
      "[CLS] what are the email addresses? [SEP]fund 70 000 to pa customers 09go to full story 09 09conoco to build 75m gas to liquids demonstration plant 09go to full story 09 09dpl to add 160 mw in ohio by 2002 09go to full story 09 09 09to view all of today s executive news headlines 20 09click here 20 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 image 09 image 09 image 09 image 09 09 09 09 09 09 09 09 09 image 09 image 09 09 copyright 2001 ft energy all rights reserved ft and financia l 20 times are trademarks of the financial times ltd 20 09 09 09 09market brief 09 09monday may 14 09 09stocks 09close 09change 09 change 09djia 0910 877 33 0956 0 20 090 52 09dj 15 util 09391 04 094 4 20 091 14 09nasdaq 092 081 92 09 25 51 09 1 21 09s p 500 091 248 92 093 3 20 090 26 09 09 09 09 09market vols 09close 09change 09 change 09amex 000 0981 841 09 9 583 0 09 10 48 09nasdaq 000 091 339 184 09 92 182 0 09 6 44 09nyse 000 09853 420 09 44 664 0 09 4 97 09 09 09 09 09commodities 09close 09change 09 change 09crude oil jun 0928 81 090 26 20 090 91 09heating oil jun 090 7525 09 0 008 09 1 05 09nat gas henry 094 435 090 157 20 093 67 09palo verde jun 09365 00 090 00 20 090 00 09cob j [SEP]\n",
      "[CLS] what are the email addresses? [SEP] 583 0 09 10 48 09nasdaq 000 091 339 184 09 92 182 0 09 6 44 09nyse 000 09853 420 09 44 664 0 09 4 97 09 09 09 09 09commodities 09close 09change 09 change 09crude oil jun 0928 81 090 26 20 090 91 09heating oil jun 090 7525 09 0 008 09 1 05 09nat gas henry 094 435 090 157 20 093 67 09palo verde jun 09365 00 090 00 20 090 00 09cob jun 09320 00 09 5 00 09 1 54 09pjm jun 0962 00 090 00 20 090 00 09 09 09 09 09dollar us 09close 09change 09 change 09australia 20 091 927 090 013 20 090 68 09canada 20 091 552 090 001 20 090 06 09germany dmark 20 092 237 090 005 20 090 22 09euro 20 090 8739 09 0 001 09 0 16 09japan _ en 20 09123 30 090 700 20 090 57 09mexico np 099 16 09 0 040 09 0 43 09uk pound 20 090 7044 09 0 0004 09 0 06 09 09 09 09 09foreign indices 09close 09change 09 change 09arg merval 09415 60 09 3 83 09 0 91 09austr all ord 093 319 20 09 7 10 09 0 21 09braz bovespa 0914236 94 09 256 26 09 1 77 09can tse 300 20 098010 09 13 67 09 0 17 09germany da [SEP]\n",
      "[CLS] what are the email addresses? [SEP] 30 090 700 20 090 57 09mexico np 099 16 09 0 040 09 0 43 09uk pound 20 090 7044 09 0 0004 09 0 06 09 09 09 09 09foreign indices 09close 09change 09 change 09arg merval 09415 60 09 3 83 09 0 91 09austr all ord 093 319 20 09 7 10 09 0 21 09braz bovespa 0914236 94 09 256 26 09 1 77 09can tse 300 20 098010 09 13 67 09 0 17 09germany dax 096064 68 09 76 34 09 1 24 09hk hangseng 0913259 17 09 377 44 09 2 77 09japan nikkei 225 20 0913873 02 09 170 90 09 1 22 09mexico ipc 20 096042 03 09 68 33 09 1 12 09uk ftse 100 095 690 50 09 206 30 09 3 50 09 09 09 09 09 09 09source yahoo tradingday com 09 09 09 09 09 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 09 image 09 09 image 09 09 image 09 09 image 09 09 09 09 image 09 09 09 09 09 09 09 09 09 09 09 09 09 09 image 09 09 09 09 image 09 09 09 09 image 09 09 09 09advertise on energy insight 20 09 09 09 09 image 09 09 09 09 09 09 09 09 09 20 market briefs xls [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:7]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n",
      "[(0, 0), (0, 4), (5, 8), (9, 12), (13, 18), (19, 28), (28, 29), (0, 0), (0, 2), (2, 5), (5, 6), (7, 13), (14, 19), (20, 22), (23, 28), (29, 33), (34, 39), (40, 42), (42, 44), (44, 45), (46, 50), (51, 62), (63, 68), (69, 76), (77, 81), (81, 82), (83, 86), (87, 91), (92, 94), (95, 97), (98, 100), (100, 102), (102, 106), (106, 107), (107, 108), (108, 111), (111, 115), (115, 116), (116, 119), (120, 124), (125, 128), (129, 133), (134, 136), (137, 140), (141, 146), (147, 151), (152, 158), (159, 166), (167, 178), (179, 181), (182, 185), (186, 192), (193, 197), (198, 202), (203, 210), (211, 213), (214, 217), (218, 220), (221, 227), (228, 235), (236, 241), (242, 243), (244, 248), (249, 255), (256, 261), (262, 264), (265, 269), (270, 274), (275, 277), (278, 284), (285, 289), (290, 292), (293, 297), (298, 309), (310, 312), (313, 320), (321, 326), (327, 331), (332, 333), (334, 338), (339, 345), (346, 350), (351, 355), (356, 357), (358, 362), (363, 365), (365, 366), (367, 368), (368, 369), (370, 372), (373, 376), (377, 380), (381, 385), (386, 394), (395, 397), (398, 401), (402, 406), (407, 410), (411, 420), (421, 425)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what What\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 38\n"
     ]
    }
   ],
   "source": [
    "answers = eval(example[\"answer\"])\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# Start token index of the current span in the text.\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# End token index of the current span in the text.\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
    "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"The answer is not in this feature.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amymiller @ hotmail. com\n",
      "amymiller@hotmail.com\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = eval(examples[\"answer\"][sample_index])\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n"
     ]
    }
   ],
   "source": [
    "features = prepare_train_features(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/maha.agro/.cache/huggingface/datasets/csv/default-4f8eb56b2794af64/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-b4dc51cfd177e222.arrow\n",
      "Loading cached processed dataset at /home/maha.agro/.cache/huggingface/datasets/csv/default-4f8eb56b2794af64/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-084f5a395b269d1a.arrow\n",
      "Loading cached processed dataset at /home/maha.agro/.cache/huggingface/datasets/csv/default-4f8eb56b2794af64/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-7608a97e83e70de4.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(prepare_train_features, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if using the fine-tuned model\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"test-email-trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if training model from scratch\n",
    "# from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-email\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"test-email-trained1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/maha.agro/nlp-703/spanbert_finetuned.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpclabs.mbzu.ae/home/maha.agro/nlp-703/spanbert_finetuned.ipynb#ch0000039vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhpclabs.mbzu.ae/home/maha.agro/nlp-703/spanbert_finetuned.ipynb#ch0000039vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m trainer\u001b[39m.\u001b[39mget_eval_dataloader():\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpclabs.mbzu.ae/home/maha.agro/nlp-703/spanbert_finetuned.ipynb#ch0000039vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpclabs.mbzu.ae/home/maha.agro/nlp-703/spanbert_finetuned.ipynb#ch0000039vscode-remote?line=4'>5</a>\u001b[0m batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(trainer\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 384]), torch.Size([8, 384]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([48, 20, 41, 50, 19, 32, 44, 16], device='cuda:0'),\n",
       " tensor([ 72,  38,  44, 134,  27,  42,  56,  34], device='cuda:0'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"Unnamed: 0\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        # for k, o in enumerate(tokenized_examples[\"offset_mapping\"])[i]:\n",
    "        #     if len(o) == 0:\n",
    "        #         print(f\"empty offset mapping found {o}\")\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd49c926f9c9493a916815bcb6dd7574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n"
     ]
    }
   ],
   "source": [
    "validation_features = dataset[\"val\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"val\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 35153\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4395' max='4395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4395/4395 15:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 10.282946, 'text': 'tjohnson@hotmail.com'},\n",
       " {'score': -2.9191556, 'text': 'com'},\n",
       " {'score': -3.3822308, 'text': 't'},\n",
       " {'score': -8.781204, 'text': 'to tjohnson@hotmail.com'},\n",
       " {'score': -8.782499, 'text': 'johnson@hotmail.com'},\n",
       " {'score': -8.796215, 'text': 'hnson@hotmail.com'},\n",
       " {'score': -8.80089, 'text': '.com'},\n",
       " {'score': -8.804186, 'text': 'son@hotmail.com'},\n",
       " {'score': -8.80521, 'text': 'hotmail.com'},\n",
       " {'score': -8.808172, 'text': 'mail.com'},\n",
       " {'score': -8.80876, 'text': '@hotmail.com'},\n",
       " {'score': -8.848381,\n",
       "  'text': ', I want to send email to tjohnson@hotmail.com'},\n",
       " {'score': -8.851818, 'text': 'email to tjohnson@hotmail.com'},\n",
       " {'score': -8.853056,\n",
       "  'text': 'a, I want to send email to tjohnson@hotmail.com'},\n",
       " {'score': -8.856844,\n",
       "  'text': 'Alaska, I want to send email to tjohnson@hotmail.com'},\n",
       " {'score': -8.858824,\n",
       "  'text': 'aska, I want to send email to tjohnson@hotmail.com'},\n",
       " {'score': -8.863724,\n",
       "  'text': 'Hey Alaska, I want to send email to tjohnson@hotmail.com'},\n",
       " {'score': -8.870407, 'text': 'send email to tjohnson@hotmail.com'},\n",
       " {'score': -22.44638, 'text': 'to t'},\n",
       " {'score': -22.513557, 'text': ', I want to send email to t'},\n",
       " {'score': -22.516994, 'text': 'email to t'},\n",
       " {'score': -22.51823, 'text': 'a, I want to send email to t'},\n",
       " {'score': -22.522018, 'text': 'Alaska, I want to send email to t'},\n",
       " {'score': -22.524, 'text': 'aska, I want to send email to t'},\n",
       " {'score': -22.5289, 'text': 'Hey Alaska, I want to send email to t'},\n",
       " {'score': -22.535583, 'text': 'send email to t'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[4].cpu().numpy()\n",
    "end_logits = output.end_logits[4].cpu().numpy()\n",
    "offset_mapping = validation_features[4][\"offset_mapping\"]\n",
    "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
    "# an example index\n",
    "context = dataset[\"val\"][4][\"context\"]\n",
    "\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "        # to part of the input_ids that are not in the context.\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "            or len(offset_mapping[start_index]) == 0\n",
    "            or len(offset_mapping[end_index]) == 0 \n",
    "        ):\n",
    "            continue\n",
    "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "\n",
    "        if len(offset_mapping[start_index]) == 0:\n",
    "            print(f\"offset_mapping for the failing example is {offset_mapping}\")\n",
    "            print(f\"start_index for the failing example is {start_index}\")\n",
    "        if len(offset_mapping[end_index]) == 0:\n",
    "            print(f\"offset_mapping for the the failing example is {offset_mapping}\")\n",
    "            print(f\"end_index for the failing example is {end_index}\")\n",
    "\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'answer_start': [36], 'text': ['tjohnson@hotmail.com']}\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"val\"][4][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = dataset[\"val\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"Unnamed: 0\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"Unnamed: 0\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # Only used if squad_v2 is True.\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                        or len(offset_mapping[start_index]) == 0\n",
    "                        or len(offset_mapping[end_index]) == 0 \n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        # predictions[example[\"Unnamed: 0\"]] = best_answer[\"text\"]\n",
    "        answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "        predictions[example[\"Unnamed: 0\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 25364 example predictions split into 35153 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3f029cce464488935b6ccbb0151546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(dataset[\"val\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric()\n",
    "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"Unnamed: 0\"], \"answers\": ex[\"answer\"]} for ex in dataset[\"val\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
